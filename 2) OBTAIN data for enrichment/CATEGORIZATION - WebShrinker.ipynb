{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebShrinker API\n",
    "Get category infromation for unique TPs from TPs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "try:\n",
    "    from urllib import urlencode\n",
    "except ImportError:\n",
    "    from urllib.parse import urlencode\n",
    "\n",
    "from base64 import urlsafe_b64encode\n",
    "import hashlib\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making a query to the Webshrinker API\n",
    "def webshrinker_categories_v3(access_key, secret_key, url=b\"\", params={}):\n",
    "    params['key'] = access_key\n",
    "\n",
    "    request = \"categories/v3/{}?{}\".format(urlsafe_b64encode(url).decode('utf-8'), urlencode(params, True))\n",
    "    request_to_sign = \"{}:{}\".format(secret_key, request).encode('utf-8')\n",
    "    signed_request = hashlib.md5(request_to_sign).hexdigest()\n",
    "\n",
    "    return \"https://api.webshrinker.com/{}&hash={}\".format(request, signed_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Load globally unique TPs\n",
    "with open('/home/ubuntu/data/processed/TPs/TPs_unique.csv') as f: #, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    list_TPs = list(reader)\n",
    "\n",
    "print('COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define API key\n",
    "access_key = \"XXX\"\n",
    "secret_key = \"YYY\"\n",
    "\n",
    "# Define path for export\n",
    "f_path = '/home/ubuntu/data/datasets_for_enrichment/categorization/'\n",
    "\n",
    "# Initialize variables\n",
    "results = []\n",
    "TPs_json_data = {}\n",
    "TP_data = []\n",
    "req_number = 0\n",
    "\n",
    "# Loop through all TPs\n",
    "for TP in list_TPs:\n",
    "    \n",
    "    req_number += 1\n",
    "    \n",
    "    # Encode TP\n",
    "    url = bytes(TP[0], encoding = 'utf-8')\n",
    "\n",
    "    # Make API request\n",
    "    api_url = webshrinker_categories_v3(access_key, secret_key, url)\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    status_code = response.status_code\n",
    "    data = response.json()\n",
    "\n",
    "    if status_code == 200:\n",
    "        # Do something with the JSON response\n",
    "        print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    elif status_code == 202:\n",
    "        # The website is being visited and the categories will be updated shortly\n",
    "        print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    elif status_code == 400:\n",
    "        # Bad or malformed HTTP request\n",
    "        print(\"Bad or malformed HTTP request\")\n",
    "        print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    elif status_code == 401:\n",
    "        # Unauthorized\n",
    "        print(\"Unauthorized - check your access and secret key permissions\")\n",
    "        print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    elif status_code == 402:\n",
    "        # Request limit reached\n",
    "        print(\"Account request limit reached\")\n",
    "        print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    else:\n",
    "        # General error occurred\n",
    "        print(\"A general error occurred, try the request again\")\n",
    "    \n",
    "    # If data successfully returned\n",
    "     if 'data' in data:\n",
    "        results.append(data)  \n",
    "        TPs_json_data[TP[0]] = data['data'][0]\n",
    "    # If no data returned\n",
    "    else:\n",
    "        special_data = {}\n",
    "        special_data['data'] = []\n",
    "        special_data['data'].append({'categories':''})\n",
    "        special_data['data'][0]['categories'] = []\n",
    "        special_data['data'][0]['categories'].append({'id': 400, 'label': 'Uncategorized_Error', 'parent': 400, \n",
    "                                                      'score': 100, 'confident': True})\n",
    "        special_data['data'][0]['url'] = TP[0]\n",
    "        results.append(special_data)  \n",
    "        TPs_json_data[TP[0]] = special_data['data'][0]\n",
    "    \n",
    "name = str(req_number)\n",
    "    \n",
    "print('COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON export completed\n",
      "TXT export completed\n",
      "CSV export completed\n"
     ]
    }
   ],
   "source": [
    "# Export all data as JSON\n",
    "json_data = json.dumps(TPs_json_data)\n",
    "with open(f_path + 'json/allDataCategories_' + name + '.json',\"w\") as jsonFilehandle:\n",
    "    jsonFilehandle.write(json_data)\n",
    "    jsonFilehandle.close()\n",
    "\n",
    "print('JSON export completed')\n",
    "\n",
    "# Prepare data to be exported as CSV and TXT\n",
    "for TP_webShrinker in results:\n",
    "\n",
    "    url = TP_webShrinker['data'][0]['url']\n",
    "\n",
    "    TP_info = [url]\n",
    "    for i in range(len(TP_webShrinker['data'][0]['categories'])):\n",
    "        ID = TP_webShrinker['data'][0]['categories'][i]['id']\n",
    "        label = TP_webShrinker['data'][0]['categories'][i]['label']\n",
    "        parent = TP_webShrinker['data'][0]['categories'][i]['parent']\n",
    "        score = TP_webShrinker['data'][0]['categories'][i]['score']\n",
    "        confident =TP_webShrinker['data'][0]['categories'][i]['confident']\n",
    "        data = [ID, label, parent, score, confident]\n",
    "        TP_info.extend(data)\n",
    "    TP_data.extend([TP_info])\n",
    "\n",
    "# Export\n",
    "with open(f_path + 'txt/webShrinker_' + name + '.txt', 'w') as filehandle:\n",
    "    for listitem in TP_data:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "print('TXT export completed')\n",
    "\n",
    "with open(f_path + 'csv/webShrinker_' + name + '.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(TP_data)\n",
    "\n",
    "print('CSV export completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
