{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich all HTTP response CSV file with extra information about first-party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    • input: .csv files of processed http_responses tables and first-party categorisation data (.csv)\n",
    "    • output: every harvest’s http_responses data enriched with first-party categorisation data (.csv)\n",
    "    • script steps:\n",
    "        1. Import libraries\n",
    "        2. Load first-party categorisation data\n",
    "        3. Iterate through all processed http_responses tables:\n",
    "            (a) Load http_responses as a DF\n",
    "            (b) Create a new dataframe by merging first-party categorisation data with responses’ DF on the corresponding FP root-domain column\n",
    "            (c) Export as the enriched responses dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading visited site category and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_cat_loc_path = '/home/ubuntu/data/datasets_for_enrichment/provided/'\n",
    "visited_cat_loc_name = 'SiteCategoriesUpdated.csv'\n",
    "\n",
    "df_visited_cat_loc = pd.read_csv(visited_cat_loc_path + visited_cat_loc_name, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopLevelDomainLookUp</th>\n",
       "      <th>Country</th>\n",
       "      <th>Europe</th>\n",
       "      <th>PublicPrivate</th>\n",
       "      <th>SiteCategory</th>\n",
       "      <th>URLtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balkanweb.com</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NonEU</td>\n",
       "      <td>Private</td>\n",
       "      <td>News</td>\n",
       "      <td>PrivateMedia NonEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joq.al</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NonEU</td>\n",
       "      <td>Private</td>\n",
       "      <td>News</td>\n",
       "      <td>PrivateMedia NonEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gazetaexpress.com</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NonEU</td>\n",
       "      <td>Private</td>\n",
       "      <td>News</td>\n",
       "      <td>PrivateMedia NonEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>syri.net</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NonEU</td>\n",
       "      <td>Private</td>\n",
       "      <td>News</td>\n",
       "      <td>PrivateMedia NonEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xing.al</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NonEU</td>\n",
       "      <td>Private</td>\n",
       "      <td>News</td>\n",
       "      <td>PrivateMedia NonEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TopLevelDomainLookUp  Country Europe PublicPrivate SiteCategory  \\\n",
       "0        balkanweb.com  Albania  NonEU       Private         News   \n",
       "1               joq.al  Albania  NonEU       Private         News   \n",
       "2    gazetaexpress.com  Albania  NonEU       Private         News   \n",
       "3             syri.net  Albania  NonEU       Private         News   \n",
       "4              xing.al  Albania  NonEU       Private         News   \n",
       "\n",
       "              URLtype  \n",
       "0  PrivateMedia NonEU  \n",
       "1  PrivateMedia NonEU  \n",
       "2  PrivateMedia NonEU  \n",
       "3  PrivateMedia NonEU  \n",
       "4  PrivateMedia NonEU  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visited_cat_loc = df_visited_cat_loc[['TopLevelDomainLookUp', 'Country', 'Europe', 'PublicPrivate', 'SiteCategory', 'URLtype']]\n",
    "df_visited_cat_loc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrichment of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file as a dataframe and return it\n",
    "def load_TP_file(f_path, f_name):\n",
    "    df = pd.read_csv(f_path + f_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating CSV 1 RES-2019-02-21.csv\n",
      "Finished generating CSV 2 RES-2020-02-07.csv\n",
      "Finished generating CSV 3 RES-2018-08-03.csv\n",
      "Finished generating CSV 4 RES-2018-07-06.csv\n",
      "Finished generating CSV 5 RES-2018-06-07.csv\n",
      "Finished generating CSV 6 RES-2019-10-16.csv\n",
      "Finished generating CSV 7 RES-2019-05-27.csv\n",
      "Finished generating CSV 8 RES-2019-10-03.csv\n",
      "Finished generating CSV 9 RES-2019-04-24.csv\n",
      "Finished generating CSV 10 RES-2018-07-17.csv\n",
      "Finished generating CSV 11 RES-2019-03-27.csv\n",
      "Finished generating CSV 12 RES-2020-03-24.csv\n",
      "Finished generating CSV 13 RES-2019-12-22.csv\n",
      "Finished generating CSV 14 RES-2018-02-07.csv\n",
      "Finished generating CSV 15 RES-2018-05-05.csv\n",
      "Finished generating CSV 16 RES-2018-09-21.csv\n",
      "Finished generating CSV 17 RES-2019-04-12.csv\n",
      "Finished generating CSV 18 RES-2018-09-06.csv\n",
      "Finished generating CSV 19 RES-2019-08-05.csv\n",
      "Finished generating CSV 20 RES-2019-09-20.csv\n",
      "Finished generating CSV 21 RES-2018-02-14.csv\n",
      "Finished generating CSV 22 RES-2018-06-01.csv\n",
      "Finished generating CSV 23 RES-2018-06-18.csv\n",
      "Finished generating CSV 24 RES-2019-11-28.csv\n",
      "Finished generating CSV 25 RES-2018-05-29.csv\n",
      "Finished generating CSV 26 RES-2020-05-12.csv\n",
      "Finished generating CSV 27 RES-2019-03-12.csv\n",
      "Finished generating CSV 28 RES-2019-01-30.csv\n",
      "Finished generating CSV 29 RES-2018-09-11.csv\n",
      "Finished generating CSV 30 RES-2018-11-07.csv\n",
      "Finished generating CSV 31 RES-2018-05-19.csv\n",
      "Finished generating CSV 32 RES-2019-11-13.csv\n",
      "Finished generating CSV 33 RES-2019-10-29.csv\n",
      "Finished generating CSV 34 RES-2018-06-12.csv\n",
      "Finished generating CSV 35 RES-2019-09-02.csv\n",
      "Finished generating CSV 36 RES-2018-10-31.csv\n",
      "Finished generating CSV 37 RES-2018-12-10.csv\n",
      "Finished generating CSV 38 RES-2020-02-25.csv\n",
      "Finished generating CSV 39 RES-2018-10-09.csv\n",
      "Finished generating CSV 40 RES-2018-11-19.csv\n",
      "Finished generating CSV 41 RES-2019-06-28.csv\n",
      "Finished generating CSV 42 RES-2018-05-25.csv\n",
      "Finished generating CSV 43 RES-2019-05-29.csv\n",
      "Finished generating CSV 44 RES-2019-07-12.csv\n",
      "Finished generating CSV 45 RES-2018-04-17.csv\n",
      "Finished generating CSV 46 RES-2018-02-09.csv\n",
      "Finished generating CSV 47 RES-2018-04-09.csv\n",
      "Finished generating CSV 48 RES-2019-11-05.csv\n",
      "Finished generating CSV 49 RES-2019-06-14.csv\n",
      "Finished generating CSV 50 RES-2019-05-01.csv\n",
      "Finished generating CSV 51 RES-2018-03-29.csv\n",
      "Finished generating CSV 52 RES-2020-03-10.csv\n",
      "Finished generating CSV 53 RES-2019-04-05.csv\n",
      "Finished generating CSV 54 RES-2018-03-21.csv\n",
      "Finished generating CSV 55 RES-2018-08-31.csv\n",
      "Finished generating CSV 56 RES-2020-06-19.csv\n",
      "Finished generating CSV 57 RES-2020-06-02.csv\n",
      "Finished generating CSV 58 RES-2020-01-13.csv\n",
      "Finished generating CSV 59 RES-2020-04-07.csv\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Define path to the folder\n",
    "res_path = '/home/ubuntu/data/processed/crawls/response/'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "res_list_of_TPs = os.listdir(res_path)\n",
    "\n",
    "# Initialize variables\n",
    "i = 0\n",
    "file_lenght = []\n",
    "\n",
    "# Iterate through all filenames\n",
    "for res_file_name in res_list_of_TPs:\n",
    "    if (res_file_name!='.ipynb_checkpoints'):\n",
    "        i += 1\n",
    "        \n",
    "        # Load a harvest file with HTTP responses\n",
    "        df_res = load_TP_file(res_path, res_file_name)\n",
    "        \n",
    "        # Get number of total responses per harevest\n",
    "        total_number_res = len(df_res)\n",
    "        total_data = [res_file_name, total_number_res]\n",
    "        file_lenght.append(total_data)\n",
    "        \n",
    "        # Merge left the responses dataframe with first-party extra information on the first-party root domain columns\n",
    "        df_enriched = pd.merge(left=df_res, right=df_visited_cat_loc, how='left', left_on='RD_site_url', right_on='TopLevelDomainLookUp')\n",
    "        \n",
    "        # Export each harvest as a CSV file\n",
    "        df_enriched.to_csv('/home/ubuntu/data/processed/crawls/response_enriched/v.3/ENR-' + res_file_name, index = False, header = True)\n",
    "\n",
    "        print('Finished generating CSV', i, res_file_name)\n",
    "\n",
    "# Export the total number of responses per every harvest as CSV\n",
    "with open('/home/ubuntu/data/processed/crawls/total_respones.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(file_lenght)\n",
    "        \n",
    "print('COMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
