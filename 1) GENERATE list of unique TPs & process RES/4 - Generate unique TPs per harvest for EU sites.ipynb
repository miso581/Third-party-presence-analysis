{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate list of unique third-parties per harvest for EU/EEA sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    • input: .csv files of enriched http_responses tables\n",
    "    • output: list of unique TPs for each harvest’s responses - CSV format\n",
    "    • script steps:\n",
    "        1. Import libraries\n",
    "        2. Iterate through all enriched http_responses tables:\n",
    "            (a) Load http_responses as a DF\n",
    "            (b) Filter out all responses from non EU/EEA FP requests\n",
    "            (c) Filter out all FP-to-FP communication\n",
    "            (d) Obtain a DF of unique TPs by filtering out all duplicates\n",
    "            (e) Export the DF with unique TPs per harvest in a CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENR-RES-2019-05-29.csv\n",
      "ENR-RES-2019-04-05.csv\n",
      "ENR-RES-2019-02-21.csv\n",
      "ENR-RES-2020-05-12.csv\n",
      "ENR-RES-2018-04-17.csv\n",
      "ENR-RES-2018-06-12.csv\n",
      "ENR-RES-2019-06-14.csv\n",
      "ENR-RES-2019-04-12.csv\n",
      "ENR-RES-2018-04-09.csv\n",
      "ENR-RES-2018-11-07.csv\n",
      "ENR-RES-2018-09-21.csv\n",
      "ENR-RES-2019-11-13.csv\n",
      "ENR-RES-2018-08-03.csv\n",
      "ENR-RES-2020-06-19.csv\n",
      "ENR-RES-2019-01-30.csv\n",
      "ENR-RES-2018-02-14.csv\n",
      "ENR-RES-2019-07-12.csv\n",
      "ENR-RES-2020-02-25.csv\n",
      "ENR-RES-2018-07-17.csv\n",
      "ENR-RES-2018-10-09.csv\n",
      "ENR-RES-2018-03-21.csv\n",
      "ENR-RES-2020-06-02.csv\n",
      "ENR-RES-2018-06-01.csv\n",
      "ENR-RES-2020-01-13.csv\n",
      "ENR-RES-2018-10-31.csv\n",
      "ENR-RES-2019-11-05.csv\n",
      "ENR-RES-2018-06-18.csv\n",
      "ENR-RES-2020-03-24.csv\n",
      "ENR-RES-2019-05-01.csv\n",
      "ENR-RES-2018-12-10.csv\n",
      "ENR-RES-2018-03-29.csv\n",
      "ENR-RES-2019-05-27.csv\n",
      "ENR-RES-2020-02-07.csv\n",
      "ENR-RES-2018-06-07.csv\n",
      "ENR-RES-2018-02-07.csv\n",
      "ENR-RES-2018-07-06.csv\n",
      "ENR-RES-2018-05-19.csv\n",
      "ENR-RES-2019-12-22.csv\n",
      "ENR-RES-2018-05-29.csv\n",
      "ENR-RES-2019-03-12.csv\n",
      "ENR-RES-2019-09-02.csv\n",
      "ENR-RES-2018-11-19.csv\n",
      "ENR-RES-2020-04-07.csv\n",
      "ENR-RES-2019-04-24.csv\n",
      "ENR-RES-2018-05-05.csv\n",
      "ENR-RES-2019-06-28.csv\n",
      "ENR-RES-2018-08-31.csv\n",
      "ENR-RES-2018-05-25.csv\n",
      "ENR-RES-2020-03-10.csv\n",
      "ENR-RES-2019-10-29.csv\n",
      "ENR-RES-2018-09-06.csv\n",
      "ENR-RES-2019-08-05.csv\n",
      "ENR-RES-2018-09-11.csv\n",
      "ENR-RES-2019-11-28.csv\n",
      "ENR-RES-2019-09-20.csv\n",
      "ENR-RES-2018-02-09.csv\n",
      "ENR-RES-2019-10-03.csv\n",
      "ENR-RES-2019-03-27.csv\n",
      "ENR-RES-2019-10-16.csv\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "f_path = '/home/ubuntu/data/processed/crawls/response_enriched/v.3/'\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for f_name in os.listdir(f_path):\n",
    "    print(f_name)\n",
    "    \n",
    "    # Load the file\n",
    "    df_res = pd.read_csv(f_path + f_name)\n",
    "\n",
    "    # Obtain the harvest date from the file name\n",
    "    re_pattern = '\\d{4}\\W\\d{2}\\W\\d{2}'\n",
    "    crawl_date = re.findall(re_pattern, f_name)[0]\n",
    "\n",
    "    # Filter out all NON EU/EEA visited sites responses\n",
    "    df_res_TP_EU = df_res[df_res['Europe'].isin(['EU', 'EEA'])]\n",
    "\n",
    "    # Filter out all NON TP responses\n",
    "    TPs_res = df_res_TP_EU['RD_url'].where(df_res_TP_EU['third_party'] == True).dropna()\n",
    "    \n",
    "    # Get all unique TPs \n",
    "    unique_TP_res = TPs_res.unique()\n",
    "    df_unique_TP_res = pd.DataFrame(unique_TP_res)\n",
    "    \n",
    "    # Export\n",
    "    df_unique_TP_res.to_csv (r'/home/ubuntu/data/processed/TPs/responses_EU/EU-RES-'+ crawl_date +'.csv', index = False, header = None)\n",
    "    \n",
    "print('COMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
